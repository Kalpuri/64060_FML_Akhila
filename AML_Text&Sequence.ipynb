{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kalpuri/64060_FML_Akhila/blob/main/AML_Text%26Sequence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWbtoihWjEuY"
      },
      "source": [
        "# **Assignment 4 - Text and Sequence Data using IMDB dataset**\n",
        "\n",
        "# **Name: Akhila Kalpuri**\n",
        "\n",
        "# **Date: 04-22-2024**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xvU70Grzjm95",
        "outputId": "2af1f453-0c6a-40b8-85d6-cb3bf4967940"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.12\n",
            "  Downloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m841.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (24.3.25)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (1.62.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (0.4.26)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12)\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (18.1.1)\n",
            "Collecting numpy<1.24,>=1.22 (from tensorflow==2.12)\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (1.16.0)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12)\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12)\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12) (0.36.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.12) (0.43.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12) (1.11.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.27.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, numpy, keras, gast, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.15.0\n",
            "    Uninstalling tensorflow-estimator-2.15.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.15.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.15.0\n",
            "    Uninstalling keras-2.15.0:\n",
            "      Successfully uninstalled keras-2.15.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.4\n",
            "    Uninstalling gast-0.5.4:\n",
            "      Successfully uninstalled gast-0.5.4\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.0\n",
            "    Uninstalling google-auth-oauthlib-1.2.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.15.2\n",
            "    Uninstalling tensorboard-2.15.2:\n",
            "      Successfully uninstalled tensorboard-2.15.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.23.5 which is incompatible.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 keras-2.12.0 numpy-1.23.5 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "keras",
                  "numpy",
                  "tensorboard",
                  "tensorflow"
                ]
              },
              "id": "febdfb14b4ac411598a7744e5c6a325c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install tensorflow==2.12 #installing the 2.12 version because google colab uses this version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG4c_RUhju9N"
      },
      "source": [
        "**Loading the important libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YeK_jF9Djzz7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import logging\n",
        "logging.getLogger('tensorflow').disabled = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD7gR63Ij5GH"
      },
      "source": [
        "**Importing TensorFlow and Keras:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "c-2TE5-Fj8fv"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras import preprocessing\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Embedding, LSTM,  Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout\n",
        "from keras.models import load_model\n",
        "from keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from google.colab import files\n",
        "import re, os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIviSS5Uj_bj"
      },
      "source": [
        "#### Considering the IMDB example from Chapter 6. Re-running the example and modifying the  by **implementing a cutoff for reviews after 150 words, Validation Sample - 10000, Consider only the top 10,000 words**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLY3dt-3kUyt"
      },
      "source": [
        "### **Model 1: Basic model just using embedded layer with  Training Sample - 100**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEHOfCGukV3K"
      },
      "source": [
        "**Creating the training sample with 100 obs , validation with 10,000 obs and test with 5000 obs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "ckifB9fLkawg",
        "outputId": "1046718f-0a14-49c6-feec-3ebfd6552d1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-55155d0a44f7>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/datasets/imdb.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(path, num_words, skip_top, maxlen, seed, start_char, oov_char, index_from, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/datasets/imdb.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_words = 10000\n",
        "maxlen = 150\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words)\n",
        "train_data = pad_sequences(train_data, maxlen=maxlen)\n",
        "test_data = pad_sequences(test_data, maxlen=maxlen)\n",
        "\n",
        "\n",
        "# Combining the Training and Testing data create an entire dataset\n",
        "texts = np.concatenate((train_data, test_data), axis=0)\n",
        "labels = np.concatenate((train_labels, test_labels), axis=0)\n",
        "\n",
        "# Splitting the data into Training and Validation Samples\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, train_size=100, test_size=10000, random_state=42, stratify=labels)\n",
        "# Further split the data to get the test size of 5000 samples\n",
        "_, test_texts, _, test_labels = train_test_split(test_data, test_labels, test_size=5000, random_state=42, stratify=test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLP9zSAbkdUS"
      },
      "outputs": [],
      "source": [
        "train_texts.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQ_BnU09kfYc"
      },
      "outputs": [],
      "source": [
        "val_texts.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvfexJuikhsD"
      },
      "outputs": [],
      "source": [
        "test_texts.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0Pp0XV8klAD"
      },
      "source": [
        "**Model Building** :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CStlE604koDt"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(10000, 8, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTamy24ukuFW"
      },
      "source": [
        "**Model Execution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaU1SqRRkzip"
      },
      "outputs": [],
      "source": [
        "# Setting Callbacks\n",
        "callbacks = ModelCheckpoint(\n",
        "            filepath= \"model1.h5\",\n",
        "            save_best_only= True,\n",
        "            monitor= \"val_loss\"\n",
        "            )\n",
        "\n",
        "\n",
        "# Model Fit - Running the Model\n",
        "Model_1 = model.fit(train_texts, train_labels,\n",
        "                    epochs=30,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(val_texts, val_labels),\n",
        "                    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg57lM4bk39t"
      },
      "source": [
        "**Ploating the Accuracy and loss for training and validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNcWbJfvk6vp"
      },
      "outputs": [],
      "source": [
        "accuracy = Model_1.history['accuracy']\n",
        "val_accuracy = Model_1.history['val_accuracy']\n",
        "\n",
        "loss = Model_1.history[\"loss\"]\n",
        "val_loss = Model_1.history[\"val_loss\"]\n",
        "\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, accuracy, color=\"grey\", linestyle=\"dashed\", label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_accuracy, color=\"blue\",linestyle=\"dashed\", label=\"Validation Accuracy\")\n",
        "plt.title(\"Model_1:Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, loss, color=\"grey\", linestyle=\"dashed\", label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, color=\"blue\", linestyle=\"dashed\", label=\"Validation Loss\")\n",
        "plt.title(\"Model_1: Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-J_aCnOlANY"
      },
      "outputs": [],
      "source": [
        "test_model = load_model('model1.h5')\n",
        "Model1_Results = test_model.evaluate(test_texts,test_labels)\n",
        "print(f'Loss: {Model1_Results[0]:.3f}')\n",
        "print(f'Accuracy: {Model1_Results[1]:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzuRoaP8lCsB"
      },
      "source": [
        "The model trained with only 100 samples, utilizing solely an embedding layer, yielded an accuracy of 50.6%, a commendable outcome given the limited training dataset size. Moving forward, we intend to maintain this model architecture while progressively augmenting the"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9OzZtDilF1a"
      },
      "source": [
        "### **Model 2: Basic model just using embedded layer with Training Sample - 5,000**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwfGS5_wlH-5"
      },
      "outputs": [],
      "source": [
        "num_words = 10000\n",
        "maxlen = 150\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words)\n",
        "train_data = pad_sequences(train_data, maxlen=maxlen)\n",
        "test_data = pad_sequences(test_data, maxlen=maxlen)\n",
        "\n",
        "# Combining the Training and Testing data create an entire dataset\n",
        "texts = np.concatenate((train_data, test_data), axis=0)\n",
        "labels = np.concatenate((train_labels, test_labels), axis=0)\n",
        "\n",
        "# Splitting the data into Training and Validation Samples\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, train_size=5000, test_size=10000, random_state=42, stratify=labels)\n",
        "\n",
        "# Further split the data to get the test size of 5000 samples\n",
        "_, test_texts, _, test_labels = train_test_split(test_data, test_labels, test_size=5000, random_state=42, stratify=test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IL10SDZlK5Z"
      },
      "outputs": [],
      "source": [
        "train_texts.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s9oyYBalMxy"
      },
      "outputs": [],
      "source": [
        "val_texts.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-2cAwiqlO0s"
      },
      "outputs": [],
      "source": [
        "test_texts.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qIqGnfXlSXc"
      },
      "source": [
        "**Model Building** :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAso4Fa8lU6v"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(10000, 8, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70_gSWSylXgT"
      },
      "outputs": [],
      "source": [
        "# Setting Callbacks\n",
        "callbacks = ModelCheckpoint(\n",
        "            filepath= \"model2.h5\",\n",
        "            save_best_only= True,\n",
        "            monitor= \"val_loss\"\n",
        "            )\n",
        "\n",
        "\n",
        "# Model Fit - Running the Model\n",
        "Model_2 = model.fit(train_texts, train_labels,\n",
        "                    epochs=30,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(val_texts, val_labels),\n",
        "                    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehOcre-DlcQx"
      },
      "source": [
        "**Ploating the Accuracy and loss for training and validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOjMMOCSldMl"
      },
      "outputs": [],
      "source": [
        "accuracy = Model_2.history['accuracy']\n",
        "val_accuracy = Model_2.history['val_accuracy']\n",
        "\n",
        "loss = Model_2.history[\"loss\"]\n",
        "val_loss = Model_2.history[\"val_loss\"]\n",
        "\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, accuracy, color=\"grey\", linestyle=\"dashed\", label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_accuracy, color=\"blue\",linestyle=\"dashed\", label=\"Validation Accuracy\")\n",
        "plt.title(\"Model_2:Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, loss, color=\"grey\", linestyle=\"dashed\", label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, color=\"blue\", linestyle=\"dashed\", label=\"Validation Loss\")\n",
        "plt.title(\"Model_2: Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3sQ2sPvlg_U"
      },
      "outputs": [],
      "source": [
        "test_model = load_model('model2.h5')\n",
        "Model2_Results = test_model.evaluate(test_texts,test_labels)\n",
        "print(f'Loss: {Model2_Results[0]:.3f}')\n",
        "print(f'Accuracy: {Model2_Results[1]:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB-NON89ljkc"
      },
      "source": [
        "### **Model 3: Basic model just using embedded layer with Training Sample - 10,000**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoARX0mYlmf2"
      },
      "outputs": [],
      "source": [
        "num_words = 10000\n",
        "maxlen = 150\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words)\n",
        "train_data = pad_sequences(train_data, maxlen=maxlen)\n",
        "test_data = pad_sequences(test_data, maxlen=maxlen)\n",
        "\n",
        "# Combining the Training and Testing data create an entire dataset\n",
        "texts = np.concatenate((train_data, test_data), axis=0)\n",
        "labels = np.concatenate((train_labels, test_labels), axis=0)\n",
        "\n",
        "# Splitting the data into Training and Validation Samples\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, train_size=10000, test_size=10000, random_state=42, stratify=labels)\n",
        "\n",
        "# Further split the data to get the test size of 5000 samples\n",
        "_, test_texts, _, test_labels = train_test_split(test_data, test_labels, test_size=5000, random_state=42, stratify=test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHQtynXmlpCm"
      },
      "outputs": [],
      "source": [
        "train_texts.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiuYbGZElsgW"
      },
      "outputs": [],
      "source": [
        "val_texts.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVtGEiprludN"
      },
      "outputs": [],
      "source": [
        "test_texts.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hKNwu7qlxdS"
      },
      "source": [
        "**Model Building** :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rax8OswDlyhd"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(10000, 8, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORgh9MGSl25W"
      },
      "outputs": [],
      "source": [
        "# Setting Callbacks\n",
        "callbacks = ModelCheckpoint(\n",
        "            filepath= \"model3.h5\",\n",
        "            save_best_only= True,\n",
        "            monitor= \"val_loss\"\n",
        "            )\n",
        "\n",
        "\n",
        "# Model Fit - Running the Model\n",
        "Model_3 = model.fit(train_texts, train_labels,\n",
        "                    epochs=30,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(val_texts, val_labels),\n",
        "                    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ki6OmSvl7Q8"
      },
      "source": [
        "**Ploating the Accuracy and loss for training and validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NNEwk9Il21_"
      },
      "outputs": [],
      "source": [
        "accuracy = Model_3.history['accuracy']\n",
        "val_accuracy = Model_3.history['val_accuracy']\n",
        "\n",
        "loss = Model_3.history[\"loss\"]\n",
        "val_loss = Model_3.history[\"val_loss\"]\n",
        "\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, accuracy, color=\"grey\", linestyle=\"dashed\", label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_accuracy, color=\"blue\",linestyle=\"dashed\", label=\"Validation Accuracy\")\n",
        "plt.title(\"Model_3:Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, loss, color=\"grey\", linestyle=\"dashed\", label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, color=\"blue\", linestyle=\"dashed\", label=\"Validation Loss\")\n",
        "plt.title(\"Model_3: Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFSXhA-pl8A1"
      },
      "outputs": [],
      "source": [
        "test_model = load_model('model3.h5')\n",
        "Model3_Results = test_model.evaluate(test_texts,test_labels)\n",
        "print(f'Loss: {Model3_Results[0]:.3f}')\n",
        "print(f'Accuracy: {Model3_Results[1]:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOJAWGnCmC-a"
      },
      "source": [
        "We experimented with three basic sequential models, each incorporating an embedded layer and trained with varying sample sizes. A discernible pattern emerged from our observations: an escalation in training samples corresponded with heightened model accuracy and diminished loss. Notably, the model trained on 100 samples displayed an accuracy of 50.6% with a loss of 69.7%, whereas the model trained on 10,000 samples exhibited a substantial improvement, achieving an accuracy of 87.8% with a loss of 27.9%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbUvrdZkmGje"
      },
      "source": [
        "### **Model 4: Using convolution 1D and Embedding layer together with training sample size as 10,000**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXcegZdTmHdc"
      },
      "source": [
        "Employing Convolution 1D, we aim to assess the influence of this integration on the model's accuracy while maintaining a consistent training sample size of 10,000. To address overfitting concerns, we have integrated MaxPooling and Dropout layers into the architecture. MaxPooling is utilized to down-sample the spatial dimensions of the input, prioritizing the extraction of the most relevant features. This process aids in feature extraction and diminishes overfitting risks by emphasizing the selection of the most significant information from the convolved features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJYCatWzmOA4"
      },
      "source": [
        "**Model Building** :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRqxWFi7mP9x"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "maxlen = 150\n",
        "model.add(Embedding(10000, 8, input_length=maxlen))\n",
        "\n",
        "model.add(Conv1D(512, 3, activation='relu'))\n",
        "model.add(MaxPooling1D(3))\n",
        "\n",
        "model.add(Conv1D(256, 3, activation='relu'))\n",
        "model.add(MaxPooling1D(3))\n",
        "\n",
        "model.add(Conv1D(256, 3, activation='relu'))\n",
        "model.add(Dropout(0.8))\n",
        "model.add(MaxPooling1D(3))\n",
        "\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "# Compiling the Model\n",
        "adam = keras.optimizers.Adam(learning_rate = 0.0001)\n",
        "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la05llxGmTeA"
      },
      "outputs": [],
      "source": [
        "callbacks = ModelCheckpoint(\n",
        "            filepath= \"model4.h5\",\n",
        "            save_best_only= True,\n",
        "            monitor= \"val_loss\"\n",
        "            )\n",
        "\n",
        "Model_4 = model.fit(train_texts, train_labels,\n",
        "                    epochs=30,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(val_texts, val_labels),\n",
        "                    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4vlSrMEmX_n"
      },
      "source": [
        "**Ploating the Accuracy and loss for training and validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-0RHvKKmY6P"
      },
      "outputs": [],
      "source": [
        "accuracy = Model_4.history['accuracy']\n",
        "val_accuracy = Model_4.history['val_accuracy']\n",
        "\n",
        "loss = Model_4.history[\"loss\"]\n",
        "val_loss = Model_4.history[\"val_loss\"]\n",
        "\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, accuracy, color=\"grey\", linestyle=\"dashed\", label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_accuracy, color=\"blue\",linestyle=\"dashed\", label=\"Validation Accuracy\")\n",
        "plt.title(\"Model_4:Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, loss, color=\"grey\", linestyle=\"dashed\", label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, color=\"blue\", linestyle=\"dashed\", label=\"Validation Loss\")\n",
        "plt.title(\"Model_4: Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nbh7AnWpmddD"
      },
      "outputs": [],
      "source": [
        "test_model = load_model('model4.h5')\n",
        "Model4_Results = test_model.evaluate(test_texts,test_labels)\n",
        "print(f'Loss: {Model4_Results[0]:.3f}')\n",
        "print(f'Accuracy: {Model4_Results[1]:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uByFoc-Tmeap"
      },
      "source": [
        "The observed decrease in model accuracy from 87.8% to 84.4% upon the incorporation of 1D convolution may be linked to the inherent sequential nature of language. Convolutional Neural Networks (CNNs) excel in discerning local patterns within data, making them particularly effective for tasks where input order holds no significance, such as image recognition. However, sentiment analysis demands a nuanced understanding of contextual and relational subtleties among words within a sentence, necessitating the capture of long-range dependencies. Hence, architectures like Recurrent Neural Networks (RNNs) are deemed more adept for sentiment analysis tasks due to their proficiency in handling sequential data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6uvwjz4mkaf"
      },
      "source": [
        "### **Model:5 A sequence model built on one-hot encoded vector sequences with LSTM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1DHXzj7mmtI"
      },
      "source": [
        "In this context, we will employ one-hot encoded vector sequences, recognized as the optimal architecture for processing text and sequence data. This technique, prevalent in Natural Language Processing (NLP), serves as a method for representing categorical data. It involves converting words or tokens into binary value vectors, where each word is represented by a unique vector wherein only one element is set to 1 (hot), while all other elements remain 0 (cold)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZx6QUIrmp4f"
      },
      "source": [
        "**Model Building** :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv85UVRWmrB9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "max_length = 150\n",
        "max_tokens = 10000\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlH2T3xrmu3v"
      },
      "outputs": [],
      "source": [
        "callbacks = ModelCheckpoint(\n",
        "            filepath= \"model5.h5\",\n",
        "            save_best_only= True,\n",
        "            monitor= \"val_loss\"\n",
        "            )\n",
        "\n",
        "Model_5 = model.fit(train_texts, train_labels,\n",
        "                    epochs=30,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(val_texts, val_labels),\n",
        "                    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeHGRFuFmy39"
      },
      "source": [
        "**Ploating the Accuracy and loss for training and validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oP3Nak58m2Q2"
      },
      "outputs": [],
      "source": [
        "accuracy = Model_5.history['accuracy']\n",
        "val_accuracy = Model_5.history['val_accuracy']\n",
        "\n",
        "loss = Model_5.history[\"loss\"]\n",
        "val_loss = Model_5.history[\"val_loss\"]\n",
        "\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, accuracy, color=\"grey\", linestyle=\"dashed\", label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_accuracy, color=\"blue\",linestyle=\"dashed\", label=\"Validation Accuracy\")\n",
        "plt.title(\"Model_5:Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, loss, color=\"grey\", linestyle=\"dashed\", label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, color=\"blue\", linestyle=\"dashed\", label=\"Validation Loss\")\n",
        "plt.title(\"Model_5: Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7lpCjb8m70r"
      },
      "outputs": [],
      "source": [
        "test_model = load_model('model5.h5')\n",
        "Model5_Results = test_model.evaluate(test_texts,test_labels)\n",
        "print(f'Loss: {Model5_Results[0]:.3f}')\n",
        "print(f'Accuracy: {Model5_Results[1]:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kMIxY-tm9I2"
      },
      "source": [
        "Observing the current state, we note that the model achieves its highest accuracy to date, reaching 88.6% with a training sample size of 10,000. Moving forward, our focus shifts to examining the effects of incorporating an LSTM model with an embedding layer. This inquiry seeks to assess how the integration of LSTM with an embedding layer, renowned for its ability to capture sequential dependencies, impacts the overall performance of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCW12cpinDFm"
      },
      "source": [
        "### **Model 6: LSTM using embedded layer with Training Sample - 10,000**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASUf7hZjnEKT"
      },
      "source": [
        "**Model Building** :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjW7pscOnGwQ"
      },
      "outputs": [],
      "source": [
        "max_length = 150\n",
        "max_tokens = 10000\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4Mey9SYnMnq"
      },
      "outputs": [],
      "source": [
        "callbacks = ModelCheckpoint(\n",
        "            filepath= \"model6.h5\",\n",
        "            save_best_only= True,\n",
        "            monitor= \"val_loss\"\n",
        "            )\n",
        "\n",
        "Model_6 = model.fit(train_texts, train_labels,\n",
        "                    epochs=30,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(val_texts, val_labels),\n",
        "                    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB-rotHDnQZA"
      },
      "source": [
        "**Ploating the Accuracy and loss for training and validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1qRWsBGnRb2"
      },
      "outputs": [],
      "source": [
        "accuracy = Model_6.history['accuracy']\n",
        "val_accuracy = Model_6.history['val_accuracy']\n",
        "\n",
        "loss = Model_6.history[\"loss\"]\n",
        "val_loss = Model_6.history[\"val_loss\"]\n",
        "\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, accuracy, color=\"grey\", linestyle=\"dashed\", label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_accuracy, color=\"blue\",linestyle=\"dashed\", label=\"Validation Accuracy\")\n",
        "plt.title(\"Model_6:Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, loss, color=\"grey\", linestyle=\"dashed\", label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, color=\"blue\", linestyle=\"dashed\", label=\"Validation Loss\")\n",
        "plt.title(\"Model_6: Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mS9T7MCnWQu"
      },
      "outputs": [],
      "source": [
        "test_model = load_model('model6.h5')\n",
        "Model6_Results = test_model.evaluate(test_texts,test_labels)\n",
        "print(f'Loss: {Model6_Results[0]:.3f}')\n",
        "print(f'Accuracy: {Model6_Results[1]:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKUQqlLgnXO3"
      },
      "source": [
        "### **Model 6.1: LSTM using embedded layer with Training Sample - 20,000**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psVAq6ftnaol"
      },
      "outputs": [],
      "source": [
        "num_words = 10000\n",
        "maxlen = 150\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words)\n",
        "train_data = pad_sequences(train_data, maxlen=maxlen)\n",
        "test_data = pad_sequences(test_data, maxlen=maxlen)\n",
        "\n",
        "# Combining the Training and Testing data create an entire dataset\n",
        "texts = np.concatenate((train_data, test_data), axis=0)\n",
        "labels = np.concatenate((train_labels, test_labels), axis=0)\n",
        "\n",
        "# Splitting the data into Training and Validation Samples\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, train_size=20000, test_size=10000, random_state=42, stratify=labels)\n",
        "\n",
        "# Further split the data to get the test size of 5000 samples\n",
        "_, test_texts, _, test_labels = train_test_split(test_data, test_labels, test_size=5000, random_state=42, stratify=test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVRuw7-DndJg"
      },
      "outputs": [],
      "source": [
        "train_texts.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mflsLkjTnfS-"
      },
      "outputs": [],
      "source": [
        "val_texts.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mphryIXcnhli"
      },
      "source": [
        "**Model Building** :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfTMIzInnoDo"
      },
      "outputs": [],
      "source": [
        "max_length = 150\n",
        "max_tokens = 10000\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5V3lFOo6nsXl"
      },
      "outputs": [],
      "source": [
        "callbacks = ModelCheckpoint(\n",
        "            filepath= \"model61.h5\",\n",
        "            save_best_only= True,\n",
        "            monitor= \"val_loss\"\n",
        "            )\n",
        "\n",
        "Model_61 = model.fit(train_texts, train_labels,\n",
        "                    epochs=30,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(val_texts, val_labels),\n",
        "                    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSie4ezsnvmw"
      },
      "source": [
        "**Ploating the Accuracy and loss for training and validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPa61iVInwX5"
      },
      "outputs": [],
      "source": [
        "accuracy = Model_61.history['accuracy']\n",
        "val_accuracy = Model_61.history['val_accuracy']\n",
        "\n",
        "loss = Model_61.history[\"loss\"]\n",
        "val_loss = Model_61.history[\"val_loss\"]\n",
        "\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, accuracy, color=\"grey\", linestyle=\"dashed\", label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_accuracy, color=\"blue\",linestyle=\"dashed\", label=\"Validation Accuracy\")\n",
        "plt.title(\"Model_6_1:Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, loss, color=\"grey\", linestyle=\"dashed\", label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, color=\"blue\", linestyle=\"dashed\", label=\"Validation Loss\")\n",
        "plt.title(\"Model_6_1: Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2xEFwHKn1sY"
      },
      "outputs": [],
      "source": [
        "test_model = load_model('model61.h5')\n",
        "Model6_Results = test_model.evaluate(test_texts,test_labels)\n",
        "print(f'Loss: {Model6_Results[0]:.3f}')\n",
        "print(f'Accuracy: {Model6_Results[1]:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBfTf6XOn4q0"
      },
      "source": [
        "Two LSTM models with embedded layers were developed, each trained with different sample sizes. Model 6.1, utilizing a larger training sample size of 20,000, demonstrated superior accuracy and lower loss compared to the alternate model. This outcome can be attributed to the model's exposure to a more extensive dataset, enabling it to discern nuanced patterns more effectively. Consequently, this enhanced learning capacity facilitated improved generalization and predictive performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwiQe6jyn7q4"
      },
      "source": [
        "### **Model 7 : LSTM with embedding layer and Masking enable with training sample size as 20,000**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxj23Vt2n8p9"
      },
      "source": [
        "**Model Building** :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wbe-g13QoCjt"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(\n",
        "    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ON7kBg3PoEwP"
      },
      "outputs": [],
      "source": [
        "callbacks = ModelCheckpoint(\n",
        "            filepath= \"model7.h5\",\n",
        "            save_best_only= True,\n",
        "            monitor= \"val_loss\"\n",
        "            )\n",
        "\n",
        "Model_7 = model.fit(train_texts, train_labels,\n",
        "                    epochs=30,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(val_texts, val_labels),\n",
        "                    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SASZnfKHoIS5"
      },
      "source": [
        "**Ploating the Accuracy and loss for training and validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqPHhLWYoJLg"
      },
      "outputs": [],
      "source": [
        "accuracy = Model_7.history['accuracy']\n",
        "val_accuracy = Model_7.history['val_accuracy']\n",
        "\n",
        "loss = Model_7.history[\"loss\"]\n",
        "val_loss = Model_7.history[\"val_loss\"]\n",
        "\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, accuracy, color=\"grey\", linestyle=\"dashed\", label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_accuracy, color=\"blue\",linestyle=\"dashed\", label=\"Validation Accuracy\")\n",
        "plt.title(\"Model_7:Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, loss, color=\"grey\", linestyle=\"dashed\", label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, color=\"blue\", linestyle=\"dashed\", label=\"Validation Loss\")\n",
        "plt.title(\"Model_7: Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgIiaO4KoOKR"
      },
      "outputs": [],
      "source": [
        "test_model = load_model('model7.h5')\n",
        "Model7_Results = test_model.evaluate(test_texts,test_labels)\n",
        "print(f'Loss: {Model7_Results[0]:.3f}')\n",
        "print(f'Accuracy: {Model7_Results[1]:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgr4trnHoQ3q"
      },
      "source": [
        "### **Model 8: Transformers with embedding layer and training sample size 20,000**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NnnLZDzoV5o"
      },
      "source": [
        "After delving into convolutional networks, one-hot encoding, and LSTM models with embedding layers, it's essential to highlight the significant impact of transformers as another architectural consideration. Renowned for their prowess in managing text and sequence data, transformers demonstrate exceptional proficiency in capturing complex, long-distance dependencies inherent within sequences. Their versatility extends to a broad spectrum of tasks including language translation, summarization, question-answering, and sentiment analysis, making them an invaluable asset in diverse applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhUMSBb1oZUO"
      },
      "source": [
        "**Model Building** :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0BI3bwqobrt"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqBXZsSboeSk"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000\n",
        "embed_dim = 150\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "maxlen = 150\n",
        "\n",
        "inputs = keras.Input(shape=(maxlen,), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiTdrJlhohW2"
      },
      "outputs": [],
      "source": [
        "callbacks = ModelCheckpoint(\n",
        "            filepath= \"model8.h5\",\n",
        "            save_best_only= True,\n",
        "            monitor= \"val_loss\"\n",
        "            )\n",
        "\n",
        "Model_8 = model.fit(train_texts, train_labels,\n",
        "                    epochs=30,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(val_texts, val_labels),\n",
        "                    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g74xiZsolXf"
      },
      "source": [
        "**Ploating the Accuracy and loss for training and validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "996RlR2vonrw"
      },
      "outputs": [],
      "source": [
        "accuracy = Model_8.history['accuracy']\n",
        "val_accuracy = Model_8.history['val_accuracy']\n",
        "\n",
        "loss = Model_8.history[\"loss\"]\n",
        "val_loss = Model_8.history[\"val_loss\"]\n",
        "\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, accuracy, color=\"grey\", linestyle=\"dashed\", label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_accuracy, color=\"blue\",linestyle=\"dashed\", label=\"Validation Accuracy\")\n",
        "plt.title(\"Model_8:Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, loss, color=\"grey\", linestyle=\"dashed\", label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, color=\"blue\", linestyle=\"dashed\", label=\"Validation Loss\")\n",
        "plt.title(\"Model_8: Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1edvtDGorO_"
      },
      "outputs": [],
      "source": [
        "test_model = load_model('model8.h5', custom_objects={'TransformerEncoder': TransformerEncoder})\n",
        "\n",
        "# Evaluate the model\n",
        "Model8_Results = test_model.evaluate(test_texts, test_labels)\n",
        "print(f'Loss: {Model8_Results[0]:.3f}')\n",
        "print(f'Accuracy: {Model8_Results[1]:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7yvTQpPou8o"
      },
      "source": [
        "## **PreTrained Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TMIHYgGowiW"
      },
      "source": [
        "GloVe, short for Global Vectors for Word Representation, stands as an unsupervised learning algorithm crafted to produce vector representations of words. This is achieved by analyzing their co-occurrence patterns within extensive text collections. Originating from the scholarly efforts of Stanford University researchers, GloVe seeks to encapsulate the semantic nuances and relational aspects of words by leveraging comprehensive statistical insights. Our data sourcing is facilitated through ai.stanford.edu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4OAzhjvo3K5"
      },
      "outputs": [],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY8KbbfIo5Xn"
      },
      "outputs": [],
      "source": [
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Up_GvyMo7b3"
      },
      "outputs": [],
      "source": [
        "imdb_dir = '/content/aclImdb'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sKx8WJ_o-Fh"
      },
      "outputs": [],
      "source": [
        "print('No. of Samples', len(texts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7KJ3YlAo-xB"
      },
      "outputs": [],
      "source": [
        "maxlen = 150  # cutting off reviews after 150 words\n",
        "training_samples = 100  # training on 100 samples\n",
        "validation_samples = 10000  # validating on 10000 samples\n",
        "max_words = 10000  # considering the top 10,000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# Splitting the data into a training set and a validation set\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]\n",
        "\n",
        "\n",
        "test_dir = os.path.join(imdb_dir, 'test')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(test_dir, label_type)\n",
        "    for fname in sorted(os.listdir(dir_name)):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "x_test = pad_sequences(sequences, maxlen=maxlen)[:5000]\n",
        "y_test = np.asarray(labels)[:5000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_QJlcRypCmz"
      },
      "outputs": [],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yf2HijoBpDT8"
      },
      "outputs": [],
      "source": [
        "x_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuJx0LDxpFJs"
      },
      "outputs": [],
      "source": [
        "x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6E5j8PapItP"
      },
      "outputs": [],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8UymAK-pNmU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpeVSthTpTQ5"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < max_words:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz1xMEeepU-H"
      },
      "source": [
        "### **Model 9:Pretrained Models with Training sample size 100- we are using GloVe model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYjpsnIJpYZa"
      },
      "source": [
        "**Model Building** :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wFa9iCfpa9a"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# Compiling the Model\n",
        "adam = keras.optimizers.Adam(learning_rate = 0.0001)\n",
        "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdSnPCagpewA"
      },
      "outputs": [],
      "source": [
        "# Setting Callbacks\n",
        "callbacks=callbacks = ModelCheckpoint(\n",
        "            filepath= \"premodel1.keras\",\n",
        "            save_best_only= True,\n",
        "            monitor= \"val_loss\"\n",
        "            )\n",
        "\n",
        "# Model Fit\n",
        "Pre_Model_1 =  model.fit(x_train, y_train,\n",
        "                     epochs=30,\n",
        "                     batch_size=32,\n",
        "                     validation_data=(x_val, y_val),\n",
        "                     callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP9AqatppqKX"
      },
      "source": [
        "**Ploating the Accuracy and loss for training and validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHmPLnxkp5gC"
      },
      "outputs": [],
      "source": [
        "accuracy = Pre_Model_1.history['accuracy']\n",
        "val_accuracy = Pre_Model_1.history['val_accuracy']\n",
        "\n",
        "loss = Pre_Model_1.history[\"loss\"]\n",
        "val_loss = Pre_Model_1.history[\"val_loss\"]\n",
        "\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, accuracy, color=\"grey\", linestyle=\"dashed\", label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_accuracy, color=\"blue\",linestyle=\"dashed\", label=\"Validation Accuracy\")\n",
        "plt.title(\"Model_9:Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, loss, color=\"grey\", linestyle=\"dashed\", label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, color=\"blue\", linestyle=\"dashed\", label=\"Validation Loss\")\n",
        "plt.title(\"Model_9: Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io0IwvTep7kt"
      },
      "outputs": [],
      "source": [
        "test_model = load_model('premodel1.keras')\n",
        "PreModel1_Results = test_model.evaluate(x_test,y_test)\n",
        "print(f'Loss: {PreModel1_Results[0]:.3f}')\n",
        "print(f'Accuracy: {PreModel1_Results[1]:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88TYePt6qEtE"
      },
      "source": [
        "### **Model 10:Pretrained Models , 4 LSTM hidden layers with Training sample size 5000**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0hQ2-N1qJAw"
      },
      "outputs": [],
      "source": [
        "maxlen = 150  # cutting off reviews after 150 words\n",
        "training_samples = 5000  # training on 1000 samples\n",
        "validation_samples = 10000  # validating on 10000 samples\n",
        "max_words = 10000  # considering the top 10,000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# Splitting the data into a training set and a validation set\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxzND7-5qSxn"
      },
      "outputs": [],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g95fwIxmqYoR"
      },
      "outputs": [],
      "source": [
        "x_val.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5nQSf2pqcJd"
      },
      "source": [
        "**Model Building** :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T87VH8UqdQc"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(LSTM(128))\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "\n",
        "# Compiling the Model\n",
        "adam = keras.optimizers.Adam(learning_rate = 0.0001)\n",
        "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bacBZbW6rDLG"
      },
      "outputs": [],
      "source": [
        "# Setting Callbacks\n",
        "callbacks = ModelCheckpoint(\n",
        "            filepath= \"premodel2.keras\",\n",
        "            save_best_only= True,\n",
        "            monitor= \"val_loss\"\n",
        "            )\n",
        "\n",
        "# Model Fit\n",
        "Pre_Model_2 =  model.fit(x_train, y_train,\n",
        "                     epochs=40,\n",
        "                     batch_size=32,\n",
        "                     validation_data=(x_val, y_val),\n",
        "                     callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X662bEJrG6F"
      },
      "source": [
        "**Ploating the Accuracy and loss for training and validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub20EXD4rHnL"
      },
      "outputs": [],
      "source": [
        "accuracy = Pre_Model_2.history['accuracy']\n",
        "val_accuracy = Pre_Model_2.history['val_accuracy']\n",
        "\n",
        "loss = Pre_Model_2.history[\"loss\"]\n",
        "val_loss = Pre_Model_2.history[\"val_loss\"]\n",
        "\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, accuracy, color=\"grey\", linestyle=\"dashed\", label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_accuracy, color=\"blue\",linestyle=\"dashed\", label=\"Validation Accuracy\")\n",
        "plt.title(\"Model_10:Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, loss, color=\"grey\", linestyle=\"dashed\", label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, color=\"blue\", linestyle=\"dashed\", label=\"Validation Loss\")\n",
        "plt.title(\"Model_10: Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9Nw70GfrMP-"
      },
      "outputs": [],
      "source": [
        "test_model = load_model('premodel2.keras')\n",
        "PreModel2_Results = test_model.evaluate(x_test,y_test)\n",
        "print(f'Loss: {PreModel2_Results[0]:.3f}')\n",
        "print(f'Accuracy: {PreModel2_Results[1]:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6bjZPPArOpE"
      },
      "source": [
        "### **Model 11:Pretrained Models , 2 LSTM hidden layer with Training sample size 15000**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjXlMaOXrRIP"
      },
      "outputs": [],
      "source": [
        "maxlen = 150  # cutting off reviews after 150 words\n",
        "training_samples = 15000  # training on 10000 samples\n",
        "validation_samples = 10000  # validating on 10000 samples\n",
        "max_words = 10000  # considering the top 10,000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "# Splitting the data into a training set and a validation set\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol-pqZrsrTtU"
      },
      "outputs": [],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPM6_6qvrUTu"
      },
      "outputs": [],
      "source": [
        "x_val.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBQmAI5JrX9R"
      },
      "source": [
        "**Model Building** :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHxtbZ7erY8V"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "\n",
        "model.add(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2))\n",
        "\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# Compiling the Model\n",
        "adam = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGHdAO3brdLJ"
      },
      "outputs": [],
      "source": [
        "# Setting Callbacks\n",
        "callbacks = ModelCheckpoint(\n",
        "            filepath= \"premodel3.h5\",\n",
        "            save_best_only= True,\n",
        "            monitor= \"val_loss\"\n",
        "            )\n",
        "\n",
        "# Model Fit\n",
        "Pre_Model_3 =  model.fit(x_train, y_train,\n",
        "                     epochs=30,\n",
        "                     batch_size=64,\n",
        "                     validation_data=(x_val, y_val),\n",
        "                     callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQDuffPgrgjh"
      },
      "source": [
        "**Ploating the Accuracy and loss for training and validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-4ikf9GriQf"
      },
      "outputs": [],
      "source": [
        "accuracy = Pre_Model_3.history['accuracy']\n",
        "val_accuracy = Pre_Model_3.history['val_accuracy']\n",
        "\n",
        "loss = Pre_Model_3.history[\"loss\"]\n",
        "val_loss = Pre_Model_3.history[\"val_loss\"]\n",
        "\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, accuracy, color=\"grey\", linestyle=\"dashed\", label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_accuracy, color=\"blue\",linestyle=\"dashed\", label=\"Validation Accuracy\")\n",
        "plt.title(\"Model_11:Accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, loss, color=\"grey\", linestyle=\"dashed\", label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss, color=\"blue\", linestyle=\"dashed\", label=\"Validation Loss\")\n",
        "plt.title(\"Model_11: Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "An9lcDX2rlHR"
      },
      "outputs": [],
      "source": [
        "test_model = load_model('premodel3.h5')\n",
        "PreModel3_Results = test_model.evaluate(x_test,y_test)\n",
        "print(f'Loss: {PreModel3_Results[0]:.3f}')\n",
        "print(f'Accuracy: {PreModel3_Results[1]:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgWtyW9ArlyW"
      },
      "source": [
        "In our experimentation with three distinct models utilizing pre-trained GloVe embeddings and varying quantities of training data, an intriguing observation emerged. As we augmented the volume of training data, there was a corresponding increase in accuracy. However, our attempt to streamline the model by diminishing the number of LSTM hidden layers, aimed at mitigating overfitting, alongside the utilization of a larger training dataset, resulted in a decline in accuracy instead of the anticipated enhancement."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMq+8rbAMuRN9gmfKiR3iR7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}